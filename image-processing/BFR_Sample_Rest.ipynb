{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search sample \n",
    "## Passing Images as Binary File References\n",
    "\n",
    "Skillsets that pass images to custom skills use a binary file reference to serialize the images before passing them to other skills. This sample demonstrates how skills can be configured to accept image inputs and return image outputs. \n",
    "\n",
    "While the other steps in this skillset, such as OCR and redaction, have relevance, the key takeaway is configuring and passing binary file references. The custom skill does the heavy lifting. Each input record contains an image that is serialized as a `Base64` encoded string. The input also contains the layout text of image, as returned from the OCR skill. Upon receiving the input, the custom skill segments the image into smaller images based on the coordinates of the layout text. It then returns a list of images, each `Base64` encoded, back to the skillset. While this is not a particularly realistic exercise, it demonstrates techniques that could be leverage in more interesting ways, such as in a [Custom Vision](https://github.com/Azure-Samples/azure-search-power-skills/tree/master/Vision/CustomVision) skill that performs useful inferences on your images.\n",
    "\n",
    "For more information about the skills used in this example, see [OCR skill](https://docs.microsoft.com/azure/search/cognitive-search-skill-ocr), [PII skill](https://docs.microsoft.com/azure/search/cognitive-search-skill-pii-detection), and [custom skills](https://docs.microsoft.com/azure/search/cognitive-search-custom-skill-web-api).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites \n",
    "\n",
    "+ [Azure subscription](https://Azure.Microsoft.com/subscription/free)\n",
    "+ [Azure Cognitive Search service](https://docs.microsoft.com/azure/search/search-create-service-portal) (get the full service endpoint and an admin API key)\n",
    "+ [Azure Blob storage service](https://docs.microsoft.com/azure/storage/common/storage-account-create) (get the connection string)\n",
    "+ [Python 3.6+](https://www.python.org/downloads/)\n",
    "+ [Jupyter Notebook](https://jupyter.org/install)\n",
    "+ [Visual Studio Code](https://code.visualstudio.com/download) with the [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) and the [Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)\n",
    "\n",
    "If you adapt this exercise to include more image files, add [Azure Cognitive Services](https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure inputs\n",
    "\n",
    "Follow the instructions in the [readme](https://github.com/Azure-Samples/azure-search-python-samples/blob/master/Image-Processing/README.md) to set up the inputs used by the indexer, data source, and skillset.\n",
    "\n",
    "Besides connection information, you will need a blob container for the sample JPEG file, and a function app that provides the code used in the custom skill. All the necessary files are provided. The `SplitImage` folder contains an Azure function that will accept an input in the [custom skill format](https://docs.microsoft.com/azure/search/cognitive-search-custom-skill-web-api#skill-inputs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the enrichment pipeline\n",
    "In the next few steps, configure the Cognitive Search enrichment pipeline, creating these objects on your search service:\n",
    "1. Create an indexer data source. The data source references a blob storage container with at least one image file.\n",
    "2. Create a skillset that performs image analysis. The skillset references a Cognitive Services account, a custom function app, and a knowledge store.\n",
    "3. Create a search index.\n",
    "4. Create an indexer to move documents from the data source to the index while invoking the skillset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Configure all required variables. Placeholder values are expressed in upper case. Replace each with the credentials from your accounts.\n",
    "\n",
    "# Replace with a full search service endpoint the format \"https://searchservicename.search.windows.net\"\n",
    "# Paste in an admin API key. Both values can be obtained from the Azure portal.\n",
    "search_service = \"https://<YOUR-SEARCH-SERVICE-NAME>.search.windows.net\"\n",
    "api_key = '<YOUR-SEARCH-ADMIN-API-KEY>'\n",
    "\n",
    "# Leave the API version and content_type as they are listed here.\n",
    "api_version = '2020-06-30'\n",
    "content_type = 'application/json'\n",
    "\n",
    "# Replace with a Cognitive Services account name and all-in-one key.\n",
    "# Required only if processing more than 20 documents\n",
    "cog_svcs_key = '' \n",
    "cog_svcs_acct = '' \n",
    "\n",
    "# Your Azure Storage account will be used for the datasource input and knowledge store output\n",
    "# Replace with a connection string to your Azure Storage account. \n",
    "STORAGECONNSTRING = \"DefaultEndpointsProtocol=https;AccountName=<YOUR-STORAGE-ACCOUNT>;AccountKey=<YOUR-ACCOUNT-KEY>;EndpointSuffix=core.windows.net\"\n",
    "# Replace with the blob container containing your image file\n",
    "datasource_container = 'bfr-sample' \n",
    "# Container where the sliced images will be projected to. Use the value provided below.\n",
    "know_store_container = \"obfuscated\"\n",
    "\n",
    "# Replace with the Function HTTP URL of the app deployed to Azure Function\n",
    "skill_uri = \"<YOUR-FUNCTION-APP-URL>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a helper function to invoke the Cognitive Search REST APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_Url(service, resource, resource_name, action, api_version):\n",
    "    if resource_name:\n",
    "        \n",
    "        if action:\n",
    "            return service + '/'+ resource + '/' + resource_name + '/' + action + '?api-version=' + api_version\n",
    "        else:\n",
    "            return service + '/'+ resource + '/' + resource_name + '?api-version=' + api_version\n",
    "    else:\n",
    "        return service + '/'+ resource + '?api-version=' + api_version\n",
    "\n",
    "\n",
    "headers = {'api-key': api_key, 'Content-Type': content_type}\n",
    "# Test out the URLs to ensure that the configuration works\n",
    "print(construct_Url(search_service, \"indexes\", \"bfr-sample\", \"analyze\", api_version))\n",
    "print(construct_Url(search_service, \"indexes\", \"bfr-sample\", None, api_version))\n",
    "print(construct_Url(search_service, \"indexers\", None, None, api_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = datasource_container\n",
    "\n",
    "datsource_def = {\n",
    "    'name': f'{datasource_container}-ds',\n",
    "    'description': f'Datasource containing files with sample images',\n",
    "    'type': 'azureblob',\n",
    "    'subtype': None,\n",
    "    'credentials': {\n",
    "        'connectionString': f'{STORAGECONNSTRING}'\n",
    "    },\n",
    "    'container': {\n",
    "        'name': f'{datasource_container}'\n",
    "    },\n",
    "\n",
    "}\n",
    "r = requests.post(construct_Url(search_service, \"datasources\", None, None, api_version), data=json.dumps(datsource_def),  headers=headers)\n",
    "print(r)\n",
    "res = r.json()\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the skillset\n",
    "\n",
    "Binary image references are passed as inputs and outputs, starting with \"/document/normalized_images/*\" in the OCR skill. OCR output is text and layout. Only the text component is passed to PIIDectection for analysis and redactive formatting. In the custom skill, the image is sliced into component parts (text and layout from OCR, and PII entity created in the PIIDetection step).\n",
    "\n",
    "Besides skills, a skillset also specifies the knowledge store projections that shape the final output in Blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skillset_name = f'{datasource_container}-ss'\n",
    "skillset_def = {\n",
    "    'name': f'{skillset_name}',\n",
    "    'description': 'Skillset to demonstrate passing images to custom skills',\n",
    "    'skills': [\n",
    "        {\n",
    "          \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
    "          \"name\": \"OCRSkill\",\n",
    "          \"description\": \"OCR Skill\",\n",
    "          \"context\": \"/document/normalized_images/*\",\n",
    "          \"textExtractionAlgorithm\": None,\n",
    "          \"lineEnding\": \"Space\",\n",
    "          \"defaultLanguageCode\": \"en\",\n",
    "          \"detectOrientation\": True,\n",
    "          \"inputs\": [\n",
    "            {\n",
    "              \"name\": \"image\",\n",
    "              \"source\": \"/document/normalized_images/*\"\n",
    "            }\n",
    "          ],\n",
    "          \"outputs\": [\n",
    "            {\n",
    "              \"name\": \"text\",\n",
    "              \"targetName\": \"text\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"layoutText\",\n",
    "              \"targetName\": \"layoutText\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"@odata.type\": \"#Microsoft.Skills.Text.PIIDetectionSkill\",\n",
    "          \"name\": \"#1\",\n",
    "          \"description\": \"\",\n",
    "          \"context\": \"/document/merged_content\",\n",
    "          \"defaultLanguageCode\": \"en\",\n",
    "          \"minimumPrecision\": 0.5,\n",
    "          \"maskingMode\": \"replace\",\n",
    "          \"maskingCharacter\": \"*\",\n",
    "          \"inputs\": [\n",
    "            {\n",
    "              \"name\": \"text\",\n",
    "              \"source\": \"/document/merged_content\"\n",
    "            }\n",
    "          ],\n",
    "          \"outputs\": [\n",
    "            {\n",
    "              \"name\": \"piiEntities\",\n",
    "              \"targetName\": \"pii_entities\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"maskedText\",\n",
    "              \"targetName\": \"masked_text\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"@odata.type\": \"#Microsoft.Skills.Custom.WebApiSkill\",\n",
    "          \"name\": \"ImageSkill\",\n",
    "          \"description\": \"Segment Images\",\n",
    "          \"context\": \"/document/normalized_images/*\",\n",
    "          \"uri\": f'{skill_uri}',\n",
    "          \"httpMethod\": \"POST\",\n",
    "          \"timeout\": \"PT30S\",\n",
    "          \"batchSize\": 1000,\n",
    "          \"degreeOfParallelism\": 1,\n",
    "          \"inputs\": [\n",
    "            {\n",
    "              \"name\": \"image\",\n",
    "              \"source\": \"/document/normalized_images/*\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"layoutText\",\n",
    "              \"source\": \"/document/normalized_images/*/layoutText\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"pii_entities\",\n",
    "              \"source\": \"/document/merged_content/pii_entities\"\n",
    "            }\n",
    "          ],\n",
    "          \"outputs\": [\n",
    "            {\n",
    "              \"name\": \"slices\",\n",
    "              \"targetName\": \"slices\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"original\",\n",
    "              \"targetName\": \"original\"\n",
    "            }\n",
    "          ],\n",
    "          \"httpHeaders\": {}\n",
    "        },\n",
    "        {\n",
    "          \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
    "          \"name\": \"MergeSkill\",\n",
    "          \"description\": \"Merge results from cracking with OCR text\",\n",
    "          \"context\": \"/document\",\n",
    "          \"insertPreTag\": \" \",\n",
    "          \"insertPostTag\": \" \",\n",
    "          \"inputs\": [\n",
    "            {\n",
    "              \"name\": \"text\",\n",
    "              \"source\": \"/document/content\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"itemsToInsert\",\n",
    "              \"source\": \"/document/normalized_images/*/text\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"offsets\",\n",
    "              \"source\": \"/document/normalized_images/*/contentOffset\"\n",
    "            }\n",
    "          ],\n",
    "          \"outputs\": [\n",
    "            {\n",
    "              \"name\": \"mergedText\",\n",
    "              \"targetName\": \"merged_content\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "    'cognitiveServices':None,\n",
    "    'knowledgeStore': {\n",
    "        'storageConnectionString': f'{STORAGECONNSTRING}',\n",
    "        'projections': [\n",
    "          {\n",
    "        \"tables\": [],\n",
    "        \"objects\": [\n",
    "          {\n",
    "            \"storageContainer\": \"layout\",\n",
    "            \"referenceKeyName\": None,\n",
    "            \"generatedKeyName\": \"layoutKey\",\n",
    "            \"source\": \"/document/normalized_images/*/layoutText\",\n",
    "            \"sourceContext\": None,\n",
    "            \"inputs\": []\n",
    "          }\n",
    "        ],\n",
    "        \"files\": [\n",
    "          {\n",
    "            \"storageContainer\": \"slices\",\n",
    "            \"referenceKeyName\": None,\n",
    "            \"generatedKeyName\": \"slicesKey\",\n",
    "            \"source\": \"/document/normalized_images/*/slices/*\",\n",
    "            \"sourceContext\": None,\n",
    "            \"inputs\": []\n",
    "          },\n",
    "          {\n",
    "            \"storageContainer\": \"images\",\n",
    "            \"referenceKeyName\": None,\n",
    "            \"generatedKeyName\": \"imageKey\",\n",
    "            \"source\": \"/document/normalized_images/*\",\n",
    "            \"sourceContext\": None,\n",
    "            \"inputs\": []\n",
    "          },\n",
    "          {\n",
    "            \"storageContainer\": f'{know_store_container}',\n",
    "            \"referenceKeyName\": None,\n",
    "            \"generatedKeyName\": \"originalKey\",\n",
    "            \"source\": \"/document/normalized_images/*/original\",\n",
    "            \"sourceContext\": None,\n",
    "            \"inputs\": []\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(construct_Url(search_service, \"skillsets\", skillset_name, None, api_version), data=json.dumps(skillset_def),  headers=headers)\n",
    "print(r)\n",
    "\n",
    "res = r.json()\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the index\n",
    "\n",
    "A search index isn't used in this exercise, but because it's an indexer requirement, you'll create one anyway. You can use Search Explorer in the Azure portal to query the index on your own. It will contain text extracted from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexname = f'{datasource_container}-idx'\n",
    "index_def = {\n",
    "    \"name\":f'{indexname}',\n",
    "      \"defaultScoringProfile\": \"\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "          \"name\": \"image_text\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"facetable\": False,\n",
    "          \"filterable\": False,\n",
    "          \"retrievable\": True,\n",
    "          \"searchable\": True,\n",
    "          \"analyzer\": \"standard.lucene\",\n",
    "          \"indexAnalyzer\": None,\n",
    "          \"searchAnalyzer\": None,\n",
    "          \"synonymMaps\": [],\n",
    "          \"fields\": []\n",
    "        },\n",
    "  \n",
    "        {\n",
    "          \"name\": \"content\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"facetable\": False,\n",
    "          \"filterable\": False,\n",
    "          \"key\": False,\n",
    "          \"retrievable\": True,\n",
    "          \"searchable\": True,\n",
    "          \"sortable\": False,\n",
    "          \"analyzer\": \"standard.lucene\",\n",
    "          \"indexAnalyzer\": None,\n",
    "          \"searchAnalyzer\": None,\n",
    "          \"synonymMaps\": [],\n",
    "          \"fields\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata_storage_content_type\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": False,\n",
    "            \"filterable\": False,\n",
    "            \"retrievable\": True,\n",
    "            \"sortable\": False,\n",
    "            \"facetable\": False,\n",
    "            \"key\": False,\n",
    "            \"indexAnalyzer\": None,\n",
    "            \"searchAnalyzer\": None,\n",
    "            \"analyzer\": None,\n",
    "            \"synonymMaps\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata_storage_size\",\n",
    "            \"type\": \"Edm.Int64\",\n",
    "            \"searchable\": False,\n",
    "            \"filterable\": False,\n",
    "            \"retrievable\": True,\n",
    "            \"sortable\": False,\n",
    "            \"facetable\": False,\n",
    "            \"key\": False,\n",
    "            \"indexAnalyzer\": None,\n",
    "            \"searchAnalyzer\": None,\n",
    "            \"analyzer\": None,\n",
    "            \"synonymMaps\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata_storage_last_modified\",\n",
    "            \"type\": \"Edm.DateTimeOffset\",\n",
    "            \"searchable\": False,\n",
    "            \"filterable\": False,\n",
    "            \"retrievable\": True,\n",
    "            \"sortable\": False,\n",
    "            \"facetable\": False,\n",
    "            \"key\": False,\n",
    "            \"indexAnalyzer\": None,\n",
    "            \"searchAnalyzer\": None,\n",
    "            \"analyzer\": None,\n",
    "            \"synonymMaps\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata_storage_content_md5\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": False,\n",
    "            \"filterable\": False,\n",
    "            \"retrievable\": True,\n",
    "            \"sortable\": False,\n",
    "            \"facetable\": False,\n",
    "            \"key\": False,\n",
    "            \"indexAnalyzer\": None,\n",
    "            \"searchAnalyzer\": None,\n",
    "            \"analyzer\": None,\n",
    "            \"synonymMaps\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata_storage_name\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": False,\n",
    "            \"filterable\": False,\n",
    "            \"retrievable\": True,\n",
    "            \"sortable\": False,\n",
    "            \"facetable\": False,\n",
    "            \"key\": False,\n",
    "            \"indexAnalyzer\": None,\n",
    "            \"searchAnalyzer\": None,\n",
    "            \"analyzer\": None,\n",
    "            \"synonymMaps\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata_storage_path\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": False,\n",
    "            \"filterable\": False,\n",
    "            \"retrievable\": True,\n",
    "            \"sortable\": False,\n",
    "            \"facetable\": False,\n",
    "            \"key\": True,\n",
    "            \"indexAnalyzer\": None,\n",
    "            \"searchAnalyzer\": None,\n",
    "            \"analyzer\": None,\n",
    "            \"synonymMaps\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata_storage_file_extension\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": False,\n",
    "            \"filterable\": False,\n",
    "            \"retrievable\": True,\n",
    "            \"sortable\": False,\n",
    "            \"facetable\": False,\n",
    "            \"key\": False,\n",
    "            \"indexAnalyzer\": None,\n",
    "            \"searchAnalyzer\": None,\n",
    "            \"analyzer\": None,\n",
    "            \"synonymMaps\": []\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "    \"scoringProfiles\": [],\n",
    "    \"corsOptions\": None,\n",
    "    \"suggesters\": [\n",
    "        {\n",
    "            \"name\": \"sg\",\n",
    "            \"searchMode\": \"analyzingInfixMatching\",\n",
    "            \"sourceFields\": [\n",
    "                \"metadata_storage_path\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"analyzers\": [],\n",
    "    \"tokenizers\": [],\n",
    "    \"tokenFilters\": [],\n",
    "    \"charFilters\": [],\n",
    "    \"encryptionKey\": None,\n",
    "    \"similarity\": None\n",
    "}\n",
    "r = requests.post(construct_Url(search_service, \"indexes\", None, None, api_version), data=json.dumps(index_def),  headers=headers)\n",
    "print(r)\n",
    "res = r.json()\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the indexer\n",
    "\n",
    "This step creates the index (you'll run it in a separate step). At run time, the indexer connects to the data source, invokes the skillset, and outputs results. This indexer is scheduled to run every two hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexername = f'{datasource_container}-idxr'\n",
    "indexer_def = {\n",
    "    \"name\": f'{indexername}',\n",
    "    \"description\": \"Indexer to enrich hotel reviews\",\n",
    "    \"dataSourceName\": f'{datasource_container}-ds',\n",
    "    \"skillsetName\": f'{datasource_container}-ss',\n",
    "    \"targetIndexName\": f'{datasource_container}-idx',\n",
    "    \"disabled\": None,\n",
    "    \"schedule\": {\n",
    "        \"interval\": \"PT2H\",\n",
    "        \"startTime\": \"0001-01-01T00:00:00Z\"\n",
    "      },\n",
    "    \"parameters\": {\n",
    "    \"batchSize\": None,\n",
    "    \"maxFailedItems\": 0,\n",
    "    \"maxFailedItemsPerBatch\": 0,\n",
    "    \"base64EncodeKeys\": None,\n",
    "    \"configuration\": {\n",
    "      \"dataToExtract\": \"contentAndMetadata\",\n",
    "      \"parsingMode\": \"default\",\n",
    "      \"imageAction\": \"generateNormalizedImages\"\n",
    "        }\n",
    "    },\n",
    "   \"fieldMappings\": [\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_path\",\n",
    "      \"targetFieldName\": \"metadata_storage_path\",\n",
    "      \"mappingFunction\": {\n",
    "        \"name\": \"base64Encode\"\n",
    "      }\n",
    "    }\n",
    "    ],\n",
    "    \"outputFieldMappings\": [\n",
    "        {\n",
    "          \"sourceFieldName\": \"/document/normalized_images/*/text\",\n",
    "          \"targetFieldName\": \"image_text\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "r = requests.post(construct_Url(search_service, \"indexers\", None, None, api_version), data=json.dumps(indexer_def), headers=headers)\n",
    "print(r)\n",
    "res = r.json()\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the indexer\n",
    "\n",
    "This step executes the indexer you just created. It will take several minutes to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(construct_Url(search_service, \"indexers\", indexername, \"run\", api_version), data=None,  headers=headers)\n",
    "print(r)\n",
    "#res = r.json()\n",
    "#print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check status\n",
    "\n",
    "The final step in this exercise is to view results. Before doing so, make sure the lastResult status message indicates \"success\", which means that the indexer completed its work successfully, and the revised image now exists in blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(construct_Url(search_service, \"indexers\", indexername, \"status\", api_version), data=None,  headers=headers)\n",
    "print(r)\n",
    "res = r.json()\n",
    "print(res[\"lastResult\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results\n",
    "The following cell downloads the output image so that you can verify skillset success. If you get an error, check the indexer status to make sure the indexer is finished and that there were no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import base64\n",
    "from azure.storage.blob import ContainerClient\n",
    "count = 0\n",
    "container = ContainerClient.from_connection_string(conn_str=STORAGECONNSTRING, container_name=know_store_container)\n",
    "blob_list = container.list_blobs()\n",
    "for blob in blob_list:\n",
    "    print(blob.name + '\\n')\n",
    "    blob_client = container.get_blob_client(blob.name)\n",
    "    with open(\"image\" + str(count) + \".jpg\", \"wb\") as my_blob:\n",
    "                download_stream = blob_client.download_blob()\n",
    "                my_blob.write(download_stream.readall())\n",
    "    count = count + 1\n",
    "    if(count == 3):\n",
    "        break\n",
    "\n",
    "Image(filename='image0.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "In this exercise, you learned how to pass images into skills and return the modified images to the skillset for further processing. \n",
    "\n",
    "As a next step, you can start from scratch and build a [custom AML Skill](https://docs.microsoft.com/azure/search/cognitive-search-aml-skill) to perform inferences on images, or use the Custom Vision service to build a skill. The power skills github repository has a [sample custom vision skill](https://github.com/Azure-Samples/azure-search-power-skills/tree/master/Vision/CustomVision) to help you get started."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
